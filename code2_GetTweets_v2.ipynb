{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77387950",
   "metadata": {},
   "source": [
    "# Getting the twitter data\n",
    "\n",
    "In order to extract twitter information for this study, we utilized the Twitter API (Application Programming Interface) through the Python library, tweepy. The API allows for the retrieval of tweets based on specific search criteria, such as keywords or hashtags. To begin, we first set up a developer account with Twitter and obtained the necessary API keys and tokens. These were then added to our Python script, allowing for access to the API. Next, we defined our search criteria and parameters. We divided our search of tweets into four big categories and the words were chosen in the biotechnology context: i) relevant news,  (covid and vaccine), ii) companies relevance in the field (\"Johnson&Johnson\", \"Eli Lilly\", \"Novo Nordisk\", \"AbbVie\", \"Merck\", \"Pfizer\", \"Roche\",\"AstraZeneca\", \"Novartis\", \"Moderna\" \"), iii) competitors, the same companies data of the others and iv)CEOs (\"Alex Gorsky\", \"David A.Rick\", \"LarsFruergaardJørgensen\", \"RichardA.Gonzalez\", \"KennethC.Frazier\", \"AlbertBourla\", \"SeverinSchwan\",\"PascalSoriot\", \"VasNarasimhan\", \"StéphaneBancel\" , \"WernerBaumann\"). Using the tweepy library, we then executed the API call to retrieve the tweets based on our defined criteria. The criteria of time was defined to retrieve tweets every hour, during the last 7 days (2021-01-14 to 2021-01-20), given the restriction that Twitter has for downloading. The tweets were returned in JSON format, which we then parsed and extracted the relevant information, such as the tweet text, user information, and creation date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc7fe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import sys\n",
    "import tweepy\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "from prettytable import PrettyTable\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0b185af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_by_query(query: str, days: int = 7, hours_range = [19,20,21,22,23,24,25,26,27], number_tweets: int = 10):\n",
    "    \"\"\"\n",
    "    Retrieve tweets based on a given query for a specified time range and days.\n",
    "    \n",
    "    Parameters:\n",
    "    query (str): The search query for the tweets (e.g. \"COVID\").\n",
    "    days (int, optional): The number of days to go back in time for the search (default is 7).\n",
    "    hours_range (list, assigned): The list of hours to go back in time within the days specified (the assigned hours are [11,12,13,14,15, 16, 17, 18, 19, 20, 21]).\n",
    "    number_tweets (int, optional): The number of tweets to retrieve per time range (default is 10).\n",
    "    \n",
    "    Returns: \n",
    "    A list of tuples containing the date range and tweets for each hour within the days specified.\n",
    "    \"\"\"\n",
    "    tweets_list = []\n",
    "    # For the asked days\n",
    "    for i in range(days):\n",
    "        #During the determined hours, in hour case the range has to covered the NY time stock market openig hours in the swiss hours time cause I am dowaload them from Switterland.\n",
    "        for hour in hours_range:\n",
    "            # Calculate the end time for the search (current time minus specified number of days and hours)\n",
    "            date_end = (datetime.utcnow() - timedelta(days=i, hours=hour, seconds=10)).replace(microsecond=0, minute=30)\n",
    "            # Calculate the start time for the search (subtracting 1 hour from the end time)\n",
    "            date_start = date_end - timedelta(hours=1) - timedelta(seconds=2)\n",
    "            # Retrieve tweets based on the query and specified start and end times\n",
    "            tweets = client.search_recent_tweets(query=query, max_results=number_tweets, start_time=parser.isoparse(date_start.isoformat()).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),end_time=parser.isoparse(date_end.isoformat()).strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "            # Check if tweets were returned\n",
    "            if tweets is not None:\n",
    "                # Append the date range and tweets to the list\n",
    "                tweets_list.append((date_start.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),date_end.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),tweets))\n",
    "            else:\n",
    "                #In case to don't be return \n",
    "                print(f\"No tweets found for query '{query}' between {date_start} and {date_end}\")\n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0039b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets_to_txt(tweets_list, file_name):\n",
    "    \"\"\"\n",
    "    This function takes in a list of tweets and saves it to a txt file.\n",
    "    \n",
    "    Parameters:\n",
    "    tweets_list (list): a list of tweets (list of tuples), where each tuple consists of:\n",
    "        - date (str): the start date of the tweet collection in the format YYYY-MM-DD\n",
    "        - dateEnd (str): the end date of the tweet collection in the format YYYY-MM-DD\n",
    "        - tweets (object): an object containing tweets (list of tweets)\n",
    "        \n",
    "    file_name (str): the name of the output file\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for item in tweets_list:\n",
    "            date = item[0]\n",
    "            dateEnd = item[1]\n",
    "            tweets = item[2]\n",
    "            \n",
    "            # Check if tweets is None or there are no data in tweets\n",
    "            if tweets is None or not tweets.data:\n",
    "                # Write \"NA\" to file\n",
    "                file.write(f\"NA\\t{date}\\t{dateEnd}\\tNA\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Loop through each tweet in the tweets data\n",
    "            for tweet in tweets.data:\n",
    "                # Replace line breaks with spaces\n",
    "                text = tweet.text.replace(\"\\n\", \" \")\n",
    "                # Write tweet data to file\n",
    "                file.write(f\"{tweet.id}\\t{date}\\t{dateEnd}\\t{text}\\n\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "861b2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys and tokens to acces to Twitter fro developers\n",
    "consumer_key = \"e3S5SnuxRoNh0a3UPkljKdVC6\"\n",
    "consumer_secret = \"PWfZjQUDQx47bdwaN3uKxDJ2ViIBy9GDLA77yjcl5UrraWKNfh\"\n",
    "access_token = \"799127098006835200-JAwJP1XPSq9bfLGIVhdcxQaRF15RlK8\"\n",
    "access_token_secret = \"jVvsvY7VWGaoTluItO9MW0VQe6R89qMi4hmGYjjBPtWaV\"\n",
    "bearer_token=\"AAAAAAAAAAAAAAAAAAAAAHZ0jgEAAAAAUzpoNhs57bXRfwvswT0xd1b2YtM%3DFUOiuTafIyJphWyuzkPqyzUCnOSS5ASx7zlwpnZm9lle7Qafo5\"\n",
    "\n",
    "client = tweepy.Client(consumer_key= consumer_key,consumer_secret= consumer_secret,access_token= access_token,access_token_secret= access_token_secret, bearer_token=bearer_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb736e",
   "metadata": {},
   "source": [
    "# Searching the tweets\n",
    "\n",
    "For the follwoing keywords:\n",
    "\n",
    "News: vaccine, covid\n",
    "\n",
    "Companies: Johnson & Johnson, Eli Lilly, Novo Nordisk, AbbVie, Merck, Pfizer, Roche, AstraZeneca, Novartis, Moderna\n",
    "\n",
    "CEOs: Joaquin Duato, David A. Ricks, Lars Fruergaard Jørgensen, Richard A. Gonzalez, Kenneth C. Frazier, Albert Bourla, Severin Schwan, Pascal Soriot, Vas Narasimhan, Stéphane Bancel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3240e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always do a test to check hours \n",
    "tweet = get_tweets_by_query(query= \"GMO\", days= 6, number_tweets=10)\n",
    "save_tweets_to_txt(tweet, \"./data/round2/GMO_16.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a88a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN NEXT 3 BLOCKS EVERY TIME YOU CAN TO COLLECT ALL DATA FROM THE 2 + 10 + 10 KEYWORDS IN THE LAST 7 DAYS\n",
    "# Honestly I am separating by block beacuse twiter complain of the numer of data dowlaod in the same IP per certain time\n",
    "#just change output file\n",
    "\n",
    "# Get the tweets vaccine\n",
    "tweetVaccine = get_tweets_by_query(query= \"vaccine\", days= 6, number_tweets=100)\n",
    "save_tweets_to_txt(tweetVaccine, \"./data/round2/tweetVaccine_200323.txt\")\n",
    "\n",
    "\n",
    "# Get the tweets covid\n",
    "tweetCovid = get_tweets_by_query(query= \"covid\", days= 6, number_tweets=100)\n",
    "save_tweets_to_txt(tweetCovid, \"./data/round2/tweetCovid_200323.txt\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d36f4d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CEO\n",
    "#CEOs=[\"Joaquin Duato\", \"David A. Rick\", \"Lars Fruergaard Jørgensen\", \"Richard A. Gonzalez\", \"Kenneth C. Frazier\", \"Albert Bourla\", \"Severin Schwan\",\"Pascal Soriot\", \"Vas Narasimhan\", \"Stéphane Bancel\"]\n",
    "CEOs=[ \"Stéphane Bancel\"]\n",
    "\n",
    "#loop to go thourght the companies and save the files \n",
    "for ceo in CEOs:\n",
    "    tweets = get_tweets_by_query(query=ceo, days= 6, number_tweets=100)\n",
    "    save_tweets_to_txt(tweets, f\"./data/round2/ceos/{ceo}_tweets_200323.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bec8a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TWEET Companies\n",
    "#companies = [\"Johnson&Johnson\", \"Eli Lilly\", \"Novo Nordisk\", \"AbbVie\", \"Merck\", \"Pfizer\", \"Roche\",\"AstraZeneca\", \"Novartis\", \"Moderna\" ]\n",
    "companies = [\"Eli Lilly\"]\n",
    "\n",
    "#loop to go thourght the companies and save the files \n",
    "for company in companies:\n",
    "    tweets = get_tweets_by_query(query=company, days= 4, number_tweets=100)\n",
    "    save_tweets_to_txt(tweets, f\"./data/round2/companies/{company}_tweets_140323.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc057fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
